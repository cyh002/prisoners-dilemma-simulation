hydra:
  run:
    dir: .
strategies: 
  - AlwaysCooperate
  - AlwaysDefect
  - RandomStrategy
  - TitForTatExtended
  - Grudger
  - Joss
  - TitForTwoTats
  - QLearningAgent1
  # - LLMAgent
  - HumanStrategy
  - MetaAgent  # a meta strategy that can choose among multiple base strategies

rounds: 200
rounds_random: false
min_rounds: 150
max_rounds: 250
shadow_probability: 0 # probability of shadowing a strategy

# Base payoff matrix. Dynamic payoff is enabled below.
payoff_matrix:
  CC: 3
  CD: 0
  DC: 5
  DD: 1
dynamic_payoffs: true   # When true, payoffs will update based on global cooperation rate.

noise: 0.10 # Noise level (probability of a mistake in the agent's decision)

# Frequency of shock events (chance per round) that temporarily double the noise.
shock_frequency: 0.02
shock_duration: 20

# Reputation parameters (how much weight reputation has when updating decisions)
reputation_weight: 0.5

rl_params:
  learning_rate: 0.1
  discount_factor: 0.9
  exploration_rate: 0.2

llm_params:
  use_api: false
  api_key: ""
  model: "local_model"
  temperature: 0.5
  extended_prompt: true

meta_agent:
  enabled: true
  base_strategies: ["TitForTatExtended", "AlwaysDefect", "RandomStrategy"]
  switch_frequency: 50   # rounds between meta decision updates

network:
  enabled: true
  type: "random"  # can be "random" or "scale_free"
  connectivity: 0.5  # probability of an edge between two agents

logging:
  log_file: "tournament.log"
  verbose: true

gui:
  enabled: true
